{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "[notice] A new release of pip is available: 24.2 -> 25.3\n",
            "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
          ]
        }
      ],
      "source": [
        "!pip install -q google-generativeai"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "5ZIqypiOI_3I"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import google.generativeai as genai\n",
        "GENAI_API_KEY = os.getenv('GENAI_API_KEY')\n",
        "\n",
        "genai.configure(api_key=GENAI_API_KEY)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "id": "uA9x9W8mI_zk",
        "outputId": "c4347716-e7ad-47c1-f6e6-f57187b20e88"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "W: Skipping acquire of configured file 'main/source/Sources' as repository 'https://r2u.stat.illinois.edu/ubuntu jammy InRelease' does not seem to provide it (sources.list entry misspelt?)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m24.1/24.1 MB\u001b[0m \u001b[31m88.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ],
      "source": [
        "# 1) Install libs\n",
        "!apt-get update -qq\n",
        "!apt-get install -y -qq tesseract-ocr\n",
        "!pip install --quiet google-generativeai pymupdf pytesseract Pillow regex\n",
        "\n",
        "# 2) Imports & config\n",
        "import os, json, textwrap\n",
        "# from google.colab import files\n",
        "import google.generativeai as genai\n",
        "from PIL import Image\n",
        "import pytesseract\n",
        "import fitz  # pymupdf\n",
        "import io, re"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "7AeA1ZuoKgZD"
      },
      "outputs": [],
      "source": [
        "# 2) Set your Gemini (Generative AI) API key\n",
        "genai.configure(api_key=GENAI_API_KEY)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "s_-o6o27KXz2"
      },
      "outputs": [],
      "source": [
        " # 3) helper: convert first page of PDF to PIL.Image (if PDF)\n",
        "def pdf_first_page_to_pil(pdf_bytes, zoom=2.0):\n",
        "    doc = fitz.open(stream=pdf_bytes, filetype=\"pdf\")\n",
        "    page = doc.load_page(0)\n",
        "    mat = fitz.Matrix(zoom, zoom)\n",
        "    pix = page.get_pixmap(matrix=mat, alpha=False)\n",
        "    img = Image.frombytes(\"RGB\", [pix.width, pix.height], pix.samples)\n",
        "    return img\n",
        "\n",
        "# 4) OCR helper (returns text)\n",
        "def ocr_bytes_to_text(file_bytes, filename):\n",
        "    # detect pdf by extension\n",
        "    if filename.lower().endswith(\".pdf\"):\n",
        "        img = pdf_first_page_to_pil(file_bytes)\n",
        "    else:\n",
        "        img = Image.open(io.BytesIO(file_bytes)).convert(\"RGB\")\n",
        "    # optional simple preprocessing could be added here\n",
        "    text = pytesseract.image_to_string(img, lang='eng')\n",
        "    return text\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 48,
      "metadata": {
        "id": "m-brCPS_ybvp"
      },
      "outputs": [],
      "source": [
        "import json\n",
        "import time\n",
        "\n",
        "def call_gemini(system_prompt, user_prompt, model=\"gemini-2.5-flash\",\n",
        "                max_output_tokens=512, retries=1, truncate_to=3000):\n",
        "    \"\"\"\n",
        "    Robust replacement for the Gemini call in Colab.\n",
        "    - Returns a string: the model text on success, or a JSON-stringified error object on failure.\n",
        "    - Keeps same simple call shape so you can drop it in place of your old function.\n",
        "    \"\"\"\n",
        "    prompt_full = system_prompt + \"\\n\\n\" + user_prompt\n",
        "\n",
        "    for attempt in range(retries + 1):\n",
        "        try:\n",
        "            model_instance = genai.GenerativeModel(model_name=model)\n",
        "            # On retry, optionally send a truncated prompt to reduce safety/token issues\n",
        "            send_prompt = prompt_full if attempt == 0 else prompt_full[:truncate_to]\n",
        "\n",
        "            resp = model_instance.generate_content(\n",
        "                send_prompt,\n",
        "                generation_config=genai.types.GenerationConfig(\n",
        "                    temperature=0.0,\n",
        "                    max_output_tokens=max_output_tokens\n",
        "                )\n",
        "            )\n",
        "\n",
        "            # 1) Preferred fast accessor (may raise if no Part present)\n",
        "            try:\n",
        "                text = resp.text\n",
        "                if text is not None:\n",
        "                    return text\n",
        "            except Exception:\n",
        "                # fall through to safer inspection\n",
        "                pass\n",
        "\n",
        "            # 2) If no resp.text, check candidates array (older/newer client shapes)\n",
        "            raw = getattr(resp, \"_raw_response\", None) or getattr(resp, \"to_dict\", lambda: None)()\n",
        "            # If resp has candidates-like structure, try to extract first candidate text\n",
        "            try:\n",
        "                # safe traversal for common shapes\n",
        "                candidates = raw.get(\"candidates\") if isinstance(raw, dict) else None\n",
        "                if candidates and len(candidates) > 0:\n",
        "                    c0 = candidates[0]\n",
        "                    # common nesting: c0['content'][0]['text']\n",
        "                    if isinstance(c0, dict):\n",
        "                        cont = c0.get(\"content\")\n",
        "                        if cont and isinstance(cont, list) and len(cont) > 0 and isinstance(cont[0], dict):\n",
        "                            txt = cont[0].get(\"text\")\n",
        "                            if txt:\n",
        "                                return txt\n",
        "                        # some clients put plain text in c0.get('text')\n",
        "                        if c0.get(\"text\"):\n",
        "                            return c0.get(\"text\")\n",
        "            except Exception:\n",
        "                pass\n",
        "\n",
        "            # 3) If prompt_feedback/safety exists -> return clear error JSON\n",
        "            pf = getattr(resp, \"prompt_feedback\", None)\n",
        "            if pf and getattr(pf, \"safety_ratings\", None):\n",
        "                try:\n",
        "                    ratings = pf.safety_ratings\n",
        "                    safety_reason = \", \".join(f\"{r.category.name}:{r.probability.name}\" for r in ratings)\n",
        "                except Exception:\n",
        "                    safety_reason = str(pf)\n",
        "                err = {\"error\": \"blocked_by_safety\", \"safety_reason\": safety_reason}\n",
        "                print(f\"[call_gemini] Warning: blocked by safety -> {safety_reason}\")\n",
        "                return json.dumps(err)\n",
        "\n",
        "            # 4) Try to find any 'text' anywhere in raw response as last fallback\n",
        "            def _find_text(obj):\n",
        "                if isinstance(obj, dict):\n",
        "                    for k, v in obj.items():\n",
        "                        if k == \"text\" and isinstance(v, str):\n",
        "                            return v\n",
        "                        res = _find_text(v)\n",
        "                        if res:\n",
        "                            return res\n",
        "                elif isinstance(obj, list):\n",
        "                    for e in obj:\n",
        "                        res = _find_text(e)\n",
        "                        if res:\n",
        "                            return res\n",
        "                return None\n",
        "\n",
        "            found = _find_text(raw) if raw is not None else None\n",
        "            if found:\n",
        "                return found\n",
        "\n",
        "            # 5) Nothing usable found — either retry or return diagnostic\n",
        "            if attempt < retries:\n",
        "                time.sleep(1 + attempt * 2)\n",
        "                continue\n",
        "\n",
        "            # return diagnostic raw for debugging as JSON\n",
        "            return json.dumps({\"error\": \"no_content_generated\", \"raw_response_preview\": str(raw)[:2000]})\n",
        "\n",
        "        except Exception as exc:\n",
        "            # network / API error -> retry if possible, else return error JSON\n",
        "            if attempt < retries:\n",
        "                time.sleep(1 + attempt * 2)\n",
        "                continue\n",
        "            return json.dumps({\"error\": \"exception_calling_api\", \"details\": str(exc)})\n",
        "\n",
        "    # fallback (shouldn't be reached)\n",
        "    return json.dumps({\"error\": \"unknown_failure\"})\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 34,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 164
        },
        "id": "gb-vxzAIKchI",
        "outputId": "be78fdf0-6ae9-4377-ee74-a7b27b89a11d"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Upload up to 3 files (PDF or image).\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "\n",
              "     <input type=\"file\" id=\"files-5b448c15-8f42-42f4-b0d6-1da3cfe42fe9\" name=\"files[]\" multiple disabled\n",
              "        style=\"border:none\" />\n",
              "     <output id=\"result-5b448c15-8f42-42f4-b0d6-1da3cfe42fe9\">\n",
              "      Upload widget is only available when the cell has been executed in the\n",
              "      current browser session. Please rerun this cell to enable.\n",
              "      </output>\n",
              "      <script>// Copyright 2017 Google LLC\n",
              "//\n",
              "// Licensed under the Apache License, Version 2.0 (the \"License\");\n",
              "// you may not use this file except in compliance with the License.\n",
              "// You may obtain a copy of the License at\n",
              "//\n",
              "//      http://www.apache.org/licenses/LICENSE-2.0\n",
              "//\n",
              "// Unless required by applicable law or agreed to in writing, software\n",
              "// distributed under the License is distributed on an \"AS IS\" BASIS,\n",
              "// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
              "// See the License for the specific language governing permissions and\n",
              "// limitations under the License.\n",
              "\n",
              "/**\n",
              " * @fileoverview Helpers for google.colab Python module.\n",
              " */\n",
              "(function(scope) {\n",
              "function span(text, styleAttributes = {}) {\n",
              "  const element = document.createElement('span');\n",
              "  element.textContent = text;\n",
              "  for (const key of Object.keys(styleAttributes)) {\n",
              "    element.style[key] = styleAttributes[key];\n",
              "  }\n",
              "  return element;\n",
              "}\n",
              "\n",
              "// Max number of bytes which will be uploaded at a time.\n",
              "const MAX_PAYLOAD_SIZE = 100 * 1024;\n",
              "\n",
              "function _uploadFiles(inputId, outputId) {\n",
              "  const steps = uploadFilesStep(inputId, outputId);\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  // Cache steps on the outputElement to make it available for the next call\n",
              "  // to uploadFilesContinue from Python.\n",
              "  outputElement.steps = steps;\n",
              "\n",
              "  return _uploadFilesContinue(outputId);\n",
              "}\n",
              "\n",
              "// This is roughly an async generator (not supported in the browser yet),\n",
              "// where there are multiple asynchronous steps and the Python side is going\n",
              "// to poll for completion of each step.\n",
              "// This uses a Promise to block the python side on completion of each step,\n",
              "// then passes the result of the previous step as the input to the next step.\n",
              "function _uploadFilesContinue(outputId) {\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  const steps = outputElement.steps;\n",
              "\n",
              "  const next = steps.next(outputElement.lastPromiseValue);\n",
              "  return Promise.resolve(next.value.promise).then((value) => {\n",
              "    // Cache the last promise value to make it available to the next\n",
              "    // step of the generator.\n",
              "    outputElement.lastPromiseValue = value;\n",
              "    return next.value.response;\n",
              "  });\n",
              "}\n",
              "\n",
              "/**\n",
              " * Generator function which is called between each async step of the upload\n",
              " * process.\n",
              " * @param {string} inputId Element ID of the input file picker element.\n",
              " * @param {string} outputId Element ID of the output display.\n",
              " * @return {!Iterable<!Object>} Iterable of next steps.\n",
              " */\n",
              "function* uploadFilesStep(inputId, outputId) {\n",
              "  const inputElement = document.getElementById(inputId);\n",
              "  inputElement.disabled = false;\n",
              "\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  outputElement.innerHTML = '';\n",
              "\n",
              "  const pickedPromise = new Promise((resolve) => {\n",
              "    inputElement.addEventListener('change', (e) => {\n",
              "      resolve(e.target.files);\n",
              "    });\n",
              "  });\n",
              "\n",
              "  const cancel = document.createElement('button');\n",
              "  inputElement.parentElement.appendChild(cancel);\n",
              "  cancel.textContent = 'Cancel upload';\n",
              "  const cancelPromise = new Promise((resolve) => {\n",
              "    cancel.onclick = () => {\n",
              "      resolve(null);\n",
              "    };\n",
              "  });\n",
              "\n",
              "  // Wait for the user to pick the files.\n",
              "  const files = yield {\n",
              "    promise: Promise.race([pickedPromise, cancelPromise]),\n",
              "    response: {\n",
              "      action: 'starting',\n",
              "    }\n",
              "  };\n",
              "\n",
              "  cancel.remove();\n",
              "\n",
              "  // Disable the input element since further picks are not allowed.\n",
              "  inputElement.disabled = true;\n",
              "\n",
              "  if (!files) {\n",
              "    return {\n",
              "      response: {\n",
              "        action: 'complete',\n",
              "      }\n",
              "    };\n",
              "  }\n",
              "\n",
              "  for (const file of files) {\n",
              "    const li = document.createElement('li');\n",
              "    li.append(span(file.name, {fontWeight: 'bold'}));\n",
              "    li.append(span(\n",
              "        `(${file.type || 'n/a'}) - ${file.size} bytes, ` +\n",
              "        `last modified: ${\n",
              "            file.lastModifiedDate ? file.lastModifiedDate.toLocaleDateString() :\n",
              "                                    'n/a'} - `));\n",
              "    const percent = span('0% done');\n",
              "    li.appendChild(percent);\n",
              "\n",
              "    outputElement.appendChild(li);\n",
              "\n",
              "    const fileDataPromise = new Promise((resolve) => {\n",
              "      const reader = new FileReader();\n",
              "      reader.onload = (e) => {\n",
              "        resolve(e.target.result);\n",
              "      };\n",
              "      reader.readAsArrayBuffer(file);\n",
              "    });\n",
              "    // Wait for the data to be ready.\n",
              "    let fileData = yield {\n",
              "      promise: fileDataPromise,\n",
              "      response: {\n",
              "        action: 'continue',\n",
              "      }\n",
              "    };\n",
              "\n",
              "    // Use a chunked sending to avoid message size limits. See b/62115660.\n",
              "    let position = 0;\n",
              "    do {\n",
              "      const length = Math.min(fileData.byteLength - position, MAX_PAYLOAD_SIZE);\n",
              "      const chunk = new Uint8Array(fileData, position, length);\n",
              "      position += length;\n",
              "\n",
              "      const base64 = btoa(String.fromCharCode.apply(null, chunk));\n",
              "      yield {\n",
              "        response: {\n",
              "          action: 'append',\n",
              "          file: file.name,\n",
              "          data: base64,\n",
              "        },\n",
              "      };\n",
              "\n",
              "      let percentDone = fileData.byteLength === 0 ?\n",
              "          100 :\n",
              "          Math.round((position / fileData.byteLength) * 100);\n",
              "      percent.textContent = `${percentDone}% done`;\n",
              "\n",
              "    } while (position < fileData.byteLength);\n",
              "  }\n",
              "\n",
              "  // All done.\n",
              "  yield {\n",
              "    response: {\n",
              "      action: 'complete',\n",
              "    }\n",
              "  };\n",
              "}\n",
              "\n",
              "scope.google = scope.google || {};\n",
              "scope.google.colab = scope.google.colab || {};\n",
              "scope.google.colab._files = {\n",
              "  _uploadFiles,\n",
              "  _uploadFilesContinue,\n",
              "};\n",
              "})(self);\n",
              "</script> "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Saving Gemini_Generated_Image_pb5toopb5toopb5t.png to Gemini_Generated_Image_pb5toopb5toopb5t.png\n",
            "Saving sample_job_application_form.pdf to sample_job_application_form (1).pdf\n",
            "[OCR] Gemini_Generated_Image_pb5toopb5toopb5t.png: extracted ~57 words\n",
            "[OCR] sample_job_application_form (1).pdf: extracted ~79 words\n"
          ]
        }
      ],
      "source": [
        "print(\"Upload up to 3 files (PDF or image).\")\n",
        "uploaded = files.upload()\n",
        "forms = {}\n",
        "for i, (fname, bytes_io) in enumerate(uploaded.items()):\n",
        "    if i >= 3: break\n",
        "    b = bytes_io\n",
        "    try:\n",
        "        text = ocr_bytes_to_text(b, fname)\n",
        "    except Exception as e:\n",
        "        text = \"\"\n",
        "    forms[fname] = text\n",
        "    print(f\"[OCR] {fname}: extracted ~{len(text.split())} words\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GfhPHMU5R1yg"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": 56,
      "metadata": {
        "id": "gnkukPyy0kS7"
      },
      "outputs": [],
      "source": [
        "import json\n",
        "import textwrap\n",
        "\n",
        "# Assumes call_gemini(system_prompt, user_prompt, model=...) is defined and robust.\n",
        "# If not, use the robust replacement we created earlier.\n",
        "\n",
        "def _label_and_truncate_forms(forms_dict, per_file_char_limit=3000):\n",
        "    \"\"\"\n",
        "    Build labeled block for prompt. Truncate each OCR text for token safety.\n",
        "    \"\"\"\n",
        "    parts = []\n",
        "    for fname, txt in forms_dict.items():\n",
        "        snippet = txt.replace(\"\\r\\n\", \"\\n\")[:per_file_char_limit]\n",
        "        parts.append(f\"--- FILE: {fname} ---\\n{snippet}\\n\")\n",
        "    return \"\\n\".join(parts)\n",
        "\n",
        "UNIFIED_SYSTEM = textwrap.dedent(\"\"\"\n",
        "You are a document-understanding assistant for an assignment.\n",
        "You will be given multiple labeled OCR texts from uploaded forms and a user QUESTION.\n",
        "Your job:\n",
        "  1) Determine whether the QUESTION targets a single form (e.g., \"What is Alex's name in file X?\")\n",
        "     or is a multi-form/horizontal question (e.g., \"Which forms request > 500000?\").\n",
        "  2) Use ONLY the provided OCR texts. Do NOT invent information.\n",
        "  3) Answer concisely. Always include provenance (file name + short snippet).\n",
        "  4) Always return EXACT JSON (no extra text). See output schema below.\n",
        "  5) If uncertain about numeric comparisons, set confidence to \"LOW\" and include raw snippet.\n",
        "  6) If question is ambiguous, include a top-level \"note\" with a short clarifying suggestion.\n",
        "\n",
        "OUTPUT SCHEMA (must follow exactly):\n",
        "If the answer is naturally a single answer (single-form question), return an object:\n",
        "{\n",
        "  \"mode\": \"single\",\n",
        "  \"file\": \"<filename_or_null>\",\n",
        "  \"answer\": \"<short answer or null>\",\n",
        "  \"evidence\": [ {\"file\":\"<filename>\",\"snippet\":\"<short text>\"} ],\n",
        "  \"confidence\": \"HIGH|MEDIUM|LOW\",\n",
        "  \"note\": \"<optional short note>\"\n",
        "}\n",
        "\n",
        "If the answer is multi-form (list/aggregation), return an array of objects (one per matching file):\n",
        "[\n",
        "  {\n",
        "    \"file\":\"<filename>\",\n",
        "    \"extracted\": { \"<field>\": <value or null>, ... },\n",
        "    \"evidence\":[ {\"snippet\":\"<short text>\"} ],\n",
        "    \"confidence\":\"HIGH|MEDIUM|LOW\"\n",
        "  },\n",
        "  ...\n",
        "]\n",
        "\n",
        "If you cannot answer anything, return an empty array [].\n",
        "\"\"\").strip()\n",
        "\n",
        "def unified_form_query(forms_dict, question, model=\"gemini-2.5-flash\",\n",
        "                       per_file_char_limit=3000, max_output_tokens=512):\n",
        "    \"\"\"\n",
        "    Unified query: ask question over one or many forms.\n",
        "    - forms_dict: {filename: ocr_text}\n",
        "    - question: user question string\n",
        "    Returns parsed JSON (python object) or raw string if parsing failed.\n",
        "    \"\"\"\n",
        "    # 1) Build labeled files block (truncated)\n",
        "    labeled_block = _label_and_truncate_forms(forms_dict, per_file_char_limit=per_file_char_limit)\n",
        "\n",
        "    # 2) Build user prompt\n",
        "    user_prompt = f\"\"\"FILES:\n",
        "{labeled_block}\n",
        "---QUESTION---\n",
        "{question}\n",
        "\n",
        "Return JSON according to the schema in system prompt. Return JSON only.\"\"\"\n",
        "\n",
        "    # 3) Call Gemini (uses your call_gemini wrapper)\n",
        "    raw_out = call_gemini(UNIFIED_SYSTEM, user_prompt, model=model, max_output_tokens=max_output_tokens)\n",
        "\n",
        "    # 4) Try to parse JSON safely\n",
        "    try:\n",
        "        parsed = json.loads(raw_out)\n",
        "        return {\"success\": True, \"result\": parsed, \"raw\": raw_out}\n",
        "    except Exception:\n",
        "        # try to find first JSON object/array substring\n",
        "        try:\n",
        "            start = raw_out.index(\"{\")\n",
        "            end = raw_out.rfind(\"}\") + 1\n",
        "            maybe = raw_out[start:end]\n",
        "            parsed = json.loads(maybe)\n",
        "            return {\"success\": True, \"result\": parsed, \"raw\": raw_out}\n",
        "        except Exception:\n",
        "            # Not parseable: return raw and retrieved forms for debugging\n",
        "            return {\"success\": False, \"error\": \"Could not parse LLM output as JSON\", \"raw\": raw_out}\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 54,
      "metadata": {
        "id": "ltYcj1gR0k_y"
      },
      "outputs": [],
      "source": [
        "# Example sample forms (replace with OCR outputs in your real pipeline)\n",
        "forms = {\n",
        "  \"job_001.pdf\": \"\"\"Job Application Form\n",
        "Full Name: Alex Johnson\n",
        "DOB: 14-Mar-1996\n",
        "Email: alex.johnson@example.com\n",
        "Skills: Python, React, SQL\n",
        "Signature: __________________\"\"\",\n",
        "\n",
        "  \"loan_001.pdf\": \"\"\"Loan Application Form\n",
        "Name: Aisha Khan\n",
        "DOB: 11-Feb-1994\n",
        "Income: 600000\n",
        "Loan Requested: 750000\n",
        "Purpose: Start small retail business selling handicrafts.\n",
        "Signature: [scanned missing]\"\"\",\n",
        "\n",
        "  \"admit_001.pdf\": \"\"\"Admission Form\n",
        "Student Name: Meera Joshi\n",
        "DOB: 02-Feb-2002\n",
        "Program: MSc Computer Science\n",
        "Marks: 85%\n",
        "Signature: Present\"\"\"\n",
        "}\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MnDOWkDP0vOj"
      },
      "outputs": [],
      "source": [
        "q1 = \"What is the applicant's name in loan_001.pdf?\"\n",
        "out1 = unified_form_query(forms, q1)\n",
        "print(json.dumps(out1, indent=2))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 58,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 548
        },
        "collapsed": true,
        "id": "wSqUir0h0x8-",
        "outputId": "bf17c01d-877b-4fd1-ecfb-121b42cb34f0"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{\n",
            "  \"success\": true,\n",
            "  \"result\": {\n",
            "    \"mode\": \"single\",\n",
            "    \"file\": \"loan_001.pdf\",\n",
            "    \"answer\": \"Aisha Khan, with an income of 600000, requested a loan of 750000, and her signature is missing.\",\n",
            "    \"evidence\": [\n",
            "      {\n",
            "        \"file\": \"loan_001.pdf\",\n",
            "        \"snippet\": \"Name: Aisha Khan\"\n",
            "      },\n",
            "      {\n",
            "        \"file\": \"loan_001.pdf\",\n",
            "        \"snippet\": \"Income: 600000\"\n",
            "      },\n",
            "      {\n",
            "        \"file\": \"loan_001.pdf\",\n",
            "        \"snippet\": \"Loan Requested: 750000\"\n",
            "      },\n",
            "      {\n",
            "        \"file\": \"loan_001.pdf\",\n",
            "        \"snippet\": \"Signature: [scanned missing]\"\n",
            "      }\n",
            "    ],\n",
            "    \"confidence\": \"HIGH\"\n",
            "  },\n",
            "  \"raw\": \"```json\\n{\\n  \\\"mode\\\": \\\"single\\\",\\n  \\\"file\\\": \\\"loan_001.pdf\\\",\\n  \\\"answer\\\": \\\"Aisha Khan, with an income of 600000, requested a loan of 750000, and her signature is missing.\\\",\\n  \\\"evidence\\\": [\\n    {\\n      \\\"file\\\": \\\"loan_001.pdf\\\",\\n      \\\"snippet\\\": \\\"Name: Aisha Khan\\\"\\n    },\\n    {\\n      \\\"file\\\": \\\"loan_001.pdf\\\",\\n      \\\"snippet\\\": \\\"Income: 600000\\\"\\n    },\\n    {\\n      \\\"file\\\": \\\"loan_001.pdf\\\",\\n      \\\"snippet\\\": \\\"Loan Requested: 750000\\\"\\n    },\\n    {\\n      \\\"file\\\": \\\"loan_001.pdf\\\",\\n      \\\"snippet\\\": \\\"Signature: [scanned missing]\\\"\\n    }\\n  ],\\n  \\\"confidence\\\": \\\"HIGH\\\"\\n}\\n```\"\n",
            "}\n"
          ]
        }
      ],
      "source": [
        "q2 = \"Summarize loan_001.pdf in one short sentence (include name, income, requested loan, any missing items).\"\n",
        "out2 = unified_form_query(forms, q2)\n",
        "print(json.dumps(out2, indent=2))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 59,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 383
        },
        "collapsed": true,
        "id": "WMvX2AtN0-MF",
        "outputId": "0dbf2b7e-3a13-4ec5-b190-e462833cfc2b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{\n",
            "  \"success\": true,\n",
            "  \"result\": {\n",
            "    \"file\": \"loan_001.pdf\",\n",
            "    \"extracted\": {\n",
            "      \"loan_requested\": 750000\n",
            "    },\n",
            "    \"evidence\": [\n",
            "      {\n",
            "        \"snippet\": \"Loan Requested: 750000\"\n",
            "      },\n",
            "      {\n",
            "        \"snippet\": \"Signature: [scanned missing]\"\n",
            "      }\n",
            "    ],\n",
            "    \"confidence\": \"HIGH\"\n",
            "  },\n",
            "  \"raw\": \"```json\\n[\\n  {\\n    \\\"file\\\": \\\"loan_001.pdf\\\",\\n    \\\"extracted\\\": {\\n      \\\"loan_requested\\\": 750000\\n    },\\n    \\\"evidence\\\": [\\n      {\\n        \\\"snippet\\\": \\\"Loan Requested: 750000\\\"\\n      },\\n      {\\n        \\\"snippet\\\": \\\"Signature: [scanned missing]\\\"\\n      }\\n    ],\\n    \\\"confidence\\\": \\\"HIGH\\\"\\n  }\\n]\\n```\"\n",
            "}\n"
          ]
        }
      ],
      "source": [
        "q3 = \"List forms where loan requested is greater than 500000 and signature appears missing. For each, return filename, loan_requested, and evidence.\"\n",
        "out3 = unified_form_query(forms, q3)\n",
        "print(json.dumps(out3, indent=2))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-Kcny25r1C7d"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.6"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
